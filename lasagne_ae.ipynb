{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batches work better than entire gradient. Full gradients seem to just stall and stop at some non-minimal point. Shaking it up with different learning method can help but this seems like just a hack.\n",
    "\n",
    "the network needs to be pretty deep to get reasonable encoding. \n",
    "\n",
    "a full gradient from a loss function that combines pca embedding and the reconstruction loss seems to help the network get near a good local minimum and avoid getting stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theano.tensor as T \n",
    "import theano\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.insert(0,\"eigengenes\")\n",
    "import load_data\n",
    "import pickle\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import climate\n",
    "climate.enable_default_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sys.argv = ['test.py', 'bthet', '2']\n",
    "print sys.argv;\n",
    "#ecoli\n",
    "#bthet\n",
    "species = sys.argv[1]\n",
    "numhidden = int(sys.argv[2])\n",
    "data =  load_data.load(species, False)[0]\n",
    "datashape =  data.shape\n",
    "#data = np.array(data,dtype=theano.config.floatX)\n",
    "#data = theano.shared(data,name = 'data', borrow = True)\n",
    "data = np.array(data,dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#results = PCA(data)\n",
    "pca = PCA(n_components=numhidden)\n",
    "pca_embedding = pca.fit_transform(data)\n",
    "pca_reconstruction = pca.inverse_transform(pca_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.matrix('inputs')\n",
    "target_var = T.matrix('targets')\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=datashape, input_var=input_var)\n",
    "\n",
    "nonlinearity=lasagne.nonlinearities.rectify\n",
    "\n",
    "midsize = 200\n",
    "\n",
    "l_hid1 = lasagne.layers.DenseLayer(l_in, num_units=midsize, \n",
    "                                   W=lasagne.init.GlorotUniform(), \n",
    "                                   nonlinearity=nonlinearity)\n",
    "\n",
    "l_hid2 = lasagne.layers.DenseLayer(l_hid1, num_units=midsize, \n",
    "                                   W=lasagne.init.GlorotUniform(), \n",
    "                                   nonlinearity=nonlinearity)\n",
    "\n",
    "l_hidden = lasagne.layers.DenseLayer(l_hid2, num_units=numhidden, \n",
    "                                     W=lasagne.init.GlorotUniform(), \n",
    "                                     nonlinearity=nonlinearity)\n",
    "\n",
    "l_hid4 = lasagne.layers.DenseLayer(l_hidden, num_units=midsize, \n",
    "                                   W=lasagne.init.GlorotUniform(), \n",
    "                                   nonlinearity=nonlinearity)\n",
    "\n",
    "l_hid5 = lasagne.layers.DenseLayer(l_hid4, num_units=midsize, \n",
    "                                   W=lasagne.init.GlorotUniform(), \n",
    "                                   nonlinearity=nonlinearity)\n",
    "\n",
    "l_out = lasagne.layers.DenseLayer( l_hid5, num_units=datashape[1], \n",
    "                                  W=lasagne.init.GlorotUniform(), \n",
    "                                  nonlinearity=nonlinearity)\n",
    "\n",
    "model_desc = \"{}x{}x{}x{}x{}x{}x{}\".format(l_hid1.input_shape[1], \n",
    "                                 l_hid2.input_shape[1],  \n",
    "                                 l_hidden.input_shape[1], \n",
    "                                 l_hid4.input_shape[1], \n",
    "                                 l_hid5.input_shape[1], \n",
    "                                 l_out.input_shape[1],\n",
    "                                 datashape[1])\n",
    "print model_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(l_out)\n",
    "hidden = lasagne.layers.get_output(l_hidden)\n",
    "loss = ((prediction - target_var)**2).sum(axis=1).mean()\n",
    "#+ ((hidden - pca_embedding)**2).sum(axis=1).mean()\n",
    "hiddenloss = loss + ((hidden - pca_embedding)**2).sum(axis=1).mean()\n",
    "\n",
    "#define how to make prediction\n",
    "ae_reconstruct = theano.function(\n",
    "    inputs=[input_var],\n",
    "    outputs=lasagne.layers.get_output(l_out),\n",
    ")\n",
    "\n",
    "#define how to output embedding\n",
    "ae_embed = theano.function(\n",
    "    inputs=[input_var],\n",
    "    outputs=lasagne.layers.get_output(l_hidden),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "\n",
    "hiddenupdates = lasagne.updates.adadelta(hiddenloss, params, learning_rate=0.01)\n",
    "\n",
    "pretrain_model = theano.function(\n",
    "    inputs=[input_var, target_var], \n",
    "    outputs=hiddenloss,\n",
    "    updates=hiddenupdates)\n",
    "\n",
    "lr = theano.shared(np.array(0.00001, dtype=theano.config.floatX))\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=lr, momentum=0.9)\n",
    "updates2 = lasagne.updates.adadelta(loss, params, learning_rate=lr)\n",
    "#updates = lasagne.updates.sgd(loss, params, learning_rate=0.0001)\n",
    "\n",
    "\n",
    "train_model = theano.function(\n",
    "    inputs=[input_var, target_var], \n",
    "    outputs=loss, \n",
    "    updates=updates)\n",
    "\n",
    "train_model2 = theano.function(\n",
    "    inputs=[input_var, target_var], \n",
    "    outputs=loss, \n",
    "    updates=updates2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]                                              \n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "print(\"Starting pretraining...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_err = pretrain_model(data,data)\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {} of {} took {:.3f}s, loss:\\t{:.6f}\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time, train_err*1))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 20000\n",
    "numbatches = 10\n",
    "lr = 0.001\n",
    "print(\"Starting batch training...\")\n",
    "# We iterate over epochs:\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch % 5000 == 0:\n",
    "        lr = lr*0.1\n",
    "        print(\"Learning rate now:{}\".format(lr))\n",
    "    train_err = 0;\n",
    "    for train,target in iterate_minibatches(data,data,len(data)/numbatches,True):\n",
    "        train_err += train_model(train,target)/2\n",
    "        train_err += train_model(train,target)/2\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch {} of {} took {:.3f}s, loss:\\t{:.6f}\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time, train_err*1/numbatches))\n",
    "        sys.stdout.flush()\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## AE reconstruction error\n",
    "ae_reconstruction = ae_reconstruct(data)\n",
    "print \"ae_reconstruction, species, {}, numhidden, {}, error, {}\".format(species, numhidden,(np.sum((ae_reconstruction - data)**2, axis=1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=numhidden)\n",
    "pca_embedding = pca.fit_transform(data)\n",
    "pca_reconstruction = pca.inverse_transform(pca_embedding)\n",
    "## PCA reconstruction error\n",
    "print \"pca_reconstruction, species, {}, numhidden, {}, error, {}\".format(species, numhidden,(np.sum((pca_reconstruction - data)**2, axis=1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl_params = lasagne.layers.get_all_param_values(l_out, trainable=True)\n",
    "pickle.dump(pkl_params, open( species + \"_\" + model_desc + \"_params.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lasagne.layers.set_all_param_values(l_out,pkl_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae_embedding = ae_embed(data)\n",
    "#ae_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pickle.dump(ae_embedding, open( species + \"_\" + model_desc + \"_ae_embedding.p\", \"wb\" ) )\n",
    "ae_embedding = pickle.load(open( species + \"_\" + model_desc + \"_ae_embedding.p\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show plot of 2d embedding\n",
    "if show:\n",
    "    %matplotlib inline\n",
    "    plt.plot(ae_embedding[:,0], ae_embedding[:,1], 'ro')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show pca of embedding\n",
    "if show:\n",
    "    %matplotlib inline\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_plot = pca.fit_transform(ae_embedding)\n",
    "    plt.plot(pca_plot[:,0], pca_plot[:,1], 'ro')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show:\n",
    "    %matplotlib inline\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_plot = pca.fit_transform(pca_embedding)\n",
    "    plt.plot(pca_plot[:,0], pca_plot[:,1], 'ro')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
